{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b317df4b",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb8ee98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00             1.9      0.076   \n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1594            6.2             0.600         0.08             2.0      0.090   \n",
       "1595            5.9             0.550         0.10             2.2      0.062   \n",
       "1596            6.3             0.510         0.13             2.3      0.076   \n",
       "1597            5.9             0.645         0.12             2.0      0.075   \n",
       "1598            6.0             0.310         0.47             3.6      0.067   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         9.4        5  \n",
       "1         9.8        5  \n",
       "2         9.8        5  \n",
       "3         9.8        6  \n",
       "4         9.4        5  \n",
       "...       ...      ...  \n",
       "1594     10.5        5  \n",
       "1595     11.2        6  \n",
       "1596     11.0        6  \n",
       "1597     10.2        5  \n",
       "1598     11.0        6  \n",
       "\n",
       "[1599 rows x 12 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# load the csv into a dataframe using the url\n",
    "df_wine_quality = pd.read_csv(filepath_or_buffer=\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\",\n",
    "                             sep=\";\")\n",
    "\n",
    "# have a look at the df\n",
    "df_wine_quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c24bfd",
   "metadata": {},
   "source": [
    "The column \"quality\" corresponds to the wine expert rating. Therefore, this column represents the target.\n",
    "\n",
    "All other columns constitute the feature vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bbfb308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# function to create a histogram of the desired df column\n",
    "def createSubplotHist(data, x, ax, bins):\n",
    "    '''\n",
    "    data is a pandas data frame\n",
    "    x is the variable to be plotted (str)\n",
    "    ax is the ax object\n",
    "    bins is the number of bins (int)\n",
    "    '''\n",
    "    sns.histplot(data=data, x=data[x], ax=ax, bins=bins)\n",
    "    ax.set_ylabel('')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    total = float(len(data))\n",
    "    addPercentages(data, x, total, ax)\n",
    "\n",
    "# function to add percentages on top of the bars\n",
    "def addPercentages(data, var, N, ax):\n",
    "    '''\n",
    "    data is a pandas data frame\n",
    "    var is the variable to be plotted (str)\n",
    "    N is the number of datapoints (float)\n",
    "    ax is the ax object\n",
    "    '''    \n",
    "    percentages = data[var].value_counts(normalize=True).sort_index() * 100\n",
    "    percentages = percentages.values\n",
    "    for i, p in enumerate(ax.patches):\n",
    "        percentage = percentages[i]\n",
    "        x = p.get_x() + p.get_width()\n",
    "        y = p.get_height()\n",
    "        tex = \"{}%\".format(percentage.round(1))\n",
    "\n",
    "        ax.annotate(tex, (x, y), ha='right', xytext=(0, 4),\n",
    "                textcoords='offset points',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "777782d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEhCAYAAACA4jApAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAipUlEQVR4nO3de5xVdb3/8dd7uIgebv4KNA0viT/SREvHToYCJgomSqKeLgglCHFEobwhSigeFD2phaQSI4geUUpMEXREOAoCXuloSccfhYWXLEKNizAMMHx+f+w142YuzGbYM4Nr3s/HYx7stdZ3rf35DvCeNd+19ncpIjAzs3QoaOwCzMwsfxzqZmYp4lA3M0sRh7qZWYo41M3MUsShbp86ktQU3nNveG/79HGoGwCSZkiKWr5uaOQaPy/paeAzu7nfD5L6P5ssL5I0bzf2HwdcspvvsVrSL3anzhqOOxT4j6zlGZJW7OlxLb2aN3YBttf4D2BK1vIDwJ/IChTgvQatqKpeQO88HOcSoGw32o8HrqqlzZPAScC6OtZUk+uA7B9A/wH8S57fw1LEoW4ARMRbwFvly5I2A2sj4qXGq6p+RMT/1sMx1wJr833cat7nrdpbWVPm4RfLmaQWksZL+qOkUkn/lPQbSZ2y2qyWdIuklyStk/SjZP2pkl6RVCLpD5J6S9ou6QdZ+3aW9Likjcm+/5U1nPED4L6k6dpdDQVJGiRpZfJeT1FpuKby8Iuk7yc1bZH0nqSfSWqVbCv/yPVPJa3O2n+qpPmSNki6rfLwS2JfSfdJ+ljS3yRNkNQ8631D0pWVantc0qLy7yVwKDCivI7Kwy+SWkv6qaS/JP19RdIZWdt7Ju9ziqRlSR//LOnimr5/9unmULfd8TPgMuAW4AwyQwOnAT+v1O4KoBi4ECiW1DVZXgP0B2YAvwaale8g6QBgKZkQGwQMJzOc8YyklmSGNyYkzfsA91ZXoKQLgPuBBcC3gD8DE2vqkKSTgOnAQ2SGdm5K3vv6pMlJyZ+TgXOzdr0oOfb5wK9qOPz3gQOAf0v2H5113FycC/wdmJ1VR3btBcDTSS23kPnevgM8JanyMNXDwKPAN4HXgCJJR+9GLfYp4eEX2x0dgCsjYnqyvFhSF2BApXYrI2J8+YKkmWTG48+NiO1kgn4HcFvWPj8CWgGnR8QHyX4vkxnX/05EPCCpfOjht+VtqjEGeDoiLk2W50s6BDi7hvYnA5uA2yKiNOnTVmAbQES8lNx88k5EvJa130ZgZERsS2r9UjXHfgc4J+nzU5LaAaMk3RIRm2qop0JEvCapFFhTwzDYWUA3oE9EzE/WFUt6EbgZmJ/V9s6IuCOp9X/I/MA4E8j7UJQ1Lp+pW84i4tsRMV3SQZK+IWkEmVDcp1LTykHRE5iXhFu5Ryq1ORV4EVgnqXkyTPFucqzTcqlP0n7Al8n8VpDt0V3s9iLQGvidpBslfRWYHhEP1PJ2q8oDfReeqNTnuUAb4Iu17Jer7sDGrEAvNws4XlKbrHUVPxQiYh3wMb7gmkoOdcuZpK9L+h3wV+A3ZIYeSoDK91H/o9LyZ6l6EXFNpeXPkBlW2VbpqyvwuRxLbJ/UUvks/u817RARS4F+wN/InOW/DKyS1K2W96rcx+pU7mP59+CgHPbNxf7VvEf2+2aH+uZKbXbg//+p5L9Uy0kydDAPeBs4MiLaR8SpwAs57P4+maGbbJWX15M5wz6xmq/Lcizzn0AAHSut3+V97RExN+nLZ4HvkvlB9WQylr8n9q+0fEDy54dZ6yr/H2y9G8f/KOuY2Q7M2m5NjEPdcvVFMiH184hYBRUX6k6n6pl6Zc8DZyXty/Wr1GZp8h5vRMTyiFgOrABuIDPEA7XcWx4RJWSGGc6ttOmbNe0j6QZJLyX7r4+IWcBPgXbJF2TOauviDGmnT4OeB2wgc6GS5HXFWXsyfPSVSsfYVZ+XAm2quSj6bTLXHbbUqWr7VHOoW67+H5mLgz+RdLqkc8icWR8HtKoUXpXdAhwCPCqpj6SRZC7kwSeBeQeZ4ZNiSf0kfZPMHS/fAH6btFmX/Nlf0mE1vNf1wMnJrYS9Jd1K1ZDP9hzwVUlFkk6TdD5wLbA0ufe8/H1PlvSvuzhOdboA9yfXH8YDI4Cbkh8+kPn+XZTcgtmHzG9ClZ9asw44QVL3ar7HT5IZLnpQ0rDke/sr4F/ZvbtsLEUc6paTiFhP5kxzf+AJ4C4yY9cXkPl3VGPgRcSbZO4+ORyYAwwDfpxs/jhp8w6ZM/LNwINkLvYVAL0i4vWk7X+TuaNjMrDT/d1Z77WAzK19hcl7fZ1dfBo0IhaTGXIpTPr1S+CVpK/lbiBzIbc4+z7zHNwBtCQT1oOBqyLiP7O2/5jMD5UpZG7zfI7M7ZjZbgY6k7l18eBKtZeRuQ7xGzK3Yv4G6AR8MyKe3I06LUXkx9lZfZPUi8xdGi9nrTuDTEAfFxG/b7TizFLG96lbQ/gacFXy6cmVZD5gdCPwvAPdLL98pm71LhmyuB74HpkhhI+Ax4AxEbGhMWszSxuHuplZivhCqZlZijjUzcxSxKFuZpYiDnUzsxRxqJuZpYhD3cwsRRzqZmYp4lA3M0sRh7qZWYo41M3MUsShbmaWIp6l0awaXbp0+RbwXytXrmzTpUuXfcnMH/9VMk95ehkYsXLlypJq9rsEuBjYl8zDPYasXLmytEuXLj8EribzyL0LVq5c+Zek/VPAFStXrnyzAbplTUCjhnqfPn3i6aefbswSzKpYvXo1hxxyCB988AFADB8+nL/97W/ccsstRARXXXXV0YceeuhFlfd75pln+MIXvsDDDz9M27ZtGTVq1Be7du06AODggw+muLiYhQsXsmLFij8DFBcXM3jwYEaPHn1mw/bQUqDGJ401aqgn/2nM9holJSVcddVVXHPNNVx5ZebhSieeeCIHH3wwBQWZ0cqjjjqKVatWVdn38ccfZ/DgwbRv3x6A8ePHs23bNgBatGhBSUkJGzdurHg9ffp07rvvvobpmDUZtY6pS/qBpEXJ10uStkgqlLRU0hJJ95Q/UFjSUEnLk3Z96798s/waN24c3/72t+nSpUvFupNPPpnDDz8cgL/+9a/cf//99OnTp8q+q1ev5sMPP2TIkCGcffbZTJ48mTZt2gBw+eWXM3DgQBYsWMCgQYOYMmUKAwYMoHXr1g3TMWsyag31iJgRET0joieZMcKRwDhgbEScQubXgH6SDky2dQN6AxMl7VNvlZvl2cyZM2nevDnnn39+tdtXrFjBgAEDuPDCCzn11FOrbN++fTvLli1j0qRJPProo6xfv56f/exnAPTu3Zu5c+cybdo0Nm/ezOuvv84555zDTTfdxNChQ33GbnmT890vkgqBL0XEVOAEYHGyqRjoReYi0rKIKE0eUrwKODbP9ZrVm8cee4w33niDfv36MWzYMLZs2UK/fv1Ys2YNTz75JIMHD+aKK65g+PDh1e7fsWNHzjjjDFq3bk3Lli0555xzeP3116u0mzhxIqNHj+aFF15g06ZNTJ06leeff5633367nntoTcHujKlfC4xPXis+eWTSRqAd0BZYn9W+fP1OJA0j8zR5DjnkkN2t16zezJ49u+L1e++9x9lnn82cOXN49tlnmTBhAtOmTaNr16417t+7d2+Ki4u54IIL2GeffVi4cGGV9s899xwHHHAARx99NM8++yzNmzdHEpLYsmVLvfXNmo6cQl1Se+CLEfFcsmpH1uY2wDpgQ/K68vqdJGf6UwEKCwv9LD3b6916661EBGPHjq1Yd/zxx3P99dczadIkAEaNGsX3vvc91q9fT//+/SkrK+NLX/oS11xzTcU+W7du5e6776aoqAjIjNXPnDmT008/nZNOOmmncXyzusrpGaWSzgF6RcTIZHkucHtELJI0BXiOzHDMAuBEYB8y9/J+OSJqPP0oLCyM5cuX73kvzMyalj2+pbEL8Oes5SuAIkktgTeB2RFRJulOYAmZsfrrdhXoZmaWfzmdqdcXn6mbmdVJjWfqnvvFzCxFHOq2V9u2fUftjVKmKfbZ8scTetlerUXzAq69e1ljl9Ggbr6kW4O+34MPPsjDDz+MJDp16sSECRNo1qwZN9xwA2+++Sb77bcf/fv3Z+DAgVX2XbduXY3tZs2axb333kvbtm2ZNGkSnTp1AmDo0KFcc801HHHEEQ3az6bCoW7WhK1YsYLp06czZ84c2rRpw6233sqkSZMoLS1lv/3246mnnqKsrIwRI0bw+c9/vsonaSdOnFhju6lTp1ZMYvbQQw8xevRoiouL6dy5swO9Hnn4xawJO+aYY5g/fz5t2rShtLSUNWvW0L59e/7whz/Qr18/mjVrRsuWLenZsyfz58+vsv+u2tU0idmIESMauptNikPdrIlr0aIFCxcupHv37rz66qv079+fY489ljlz5rBt2zY2bdrE/PnzWbt2bZV9d9XOk5g1Doe6mdGrVy9efvllLrvsMoYMGcLVV1+NJM4991xGjBhBt27daNGiRZX9rrnmmhrbeRKzxuFQN2vC3n77bbI/K3Leeefx/vvvs2nTJq666irmzZvHjBkziIhq52r6+OOPc2rnScwajkPdrAlbu3Ytl19+OR999BEAc+fO5cgjj+RXv/oVd955J5B5mM0jjzxC375VH5Ewa9asWttlT2K2detWT2JWz3z3i1kTVlhYyPDhwxk0aBDNmjWjY8eO3HXXXey///5cffXV9O3bl4hg5MiRHHtsZibt7EnMhg0bVmM78CRmjcHTBNhez/epm1XhaQLMzJoCh7qZWYo41M3MUsShbraXaYoTejXFPtcX3/1itpfxJGa2J3ymbmaWIg51M7MUcaibmaWIQ93MLEUc6mZmKeJQNzNLkZxCXdIYSS9K+q2kIZI6S1oqaYmkeyQVJO2GSlou6SVJVad0MzOzelVrqEvqCXwd6Ab0ADoBdwBjI+IUMhPL9JN0IDAyadcbmChpn/op28zMqpPLmXpv4A3gMWAuMA84AVicbC8GegFfBZZFRGlErAdWAcdWPZyZmdWXXD5R+lngUKAvcDjwBFAQn8zZuxFoB7QF1mftV75+J5KGAcOAap+QYmZmdZfLmfqHwPyI2BoRK4Et7BzWbYB1wIbkdeX1O4mIqRFRGBGFHTp0qGvdZmZWjVxCfSnQRxkHAf8C/Hcy1g5wJrAEeAU4RVIrSe2Ao4AV+S/ZzMxqUuvwS0TMk9SdTGgXACOAvwBFkloCbwKzI6JM0p1kAr4AuC4i/ABCM7MGlNMsjRFxdTWre1TTrggo2tOizMysbvzhIzOzFHGom5mliEPdzCxFHOpmZiniUDczSxGHuplZijjUzcxSxKFuZpYiDnUzsxRxqJuZpYhD3cwsRRzqZmYp4lA3M0sRh7qZWYo41M3MUsShbmaWIg51M7MUcaibmaWIQ93MLEUc6mZmKeJQNzNLEYe6mVmKONTNzFIkp1CX9JqkRcnXfZI6S1oqaYmkeyQVJO2GSlou6SVJfeu3dDMzq6x5bQ0ktQKIiJ5Z654AxkbEIklTgH6SXgRGAoVAK2CppAURUVovlZuZWRW1hjpwHLCfpGeS9tcCJwCLk+3FwBlAGbAsCfFSSauAY4FX8161mZlVK5dQ3wzcBtwLHEkmxBURkWzfCLQD2gLrs/YrX78TScOAYQCHHHJInQs3M7OqchlT/yPwYGT8EfgQOCBrextgHbAheV15/U4iYmpEFEZEYYcOHepat5mZVSOXUB8M3A4g6SAyZ+TPSOqZbD8TWAK8ApwiqZWkdsBRwIp8F2xmZjXLZfhlGjBD0lIgyIT8B0CRpJbAm8DsiCiTdCeZgC8ArouILfVUt5mZVaPWUI+IrcD3qtnUo5q2RUBRHuoyM7M68IePzMxSxKFuZpYiDnUzsxRxqJuZpYhD3cwsRRzqZmYp4lA3M0sRh7qZWYo41M3MUsShbmaWIg51M7MUcaibmaWIQ93MLEUc6mZmKeJQNzNLEYe6mVmKONTNzFLEoW5mliIOdTOzFHGom5mliEPdzCxFHOpmZimSU6hL6ijpXUlflNRZ0lJJSyTdI6kgaTNU0nJJL0nqW79lm5lZdWoNdUktgF8CJcmqO4CxEXEKIKCfpAOBkUA3oDcwUdI+9VOymZnVJJcz9duAKcD7yfIJwOLkdTHQC/gqsCwiSiNiPbAKODbPtZqZWS12GeqSfgCsjYj52asjIpLXG4F2QFtgfVab8vXVHXNYMkyzfO3atXUu3MzMqqrtTH0wcLqkRcCXgQeAjlnb2wDrgA3J68rrq4iIqRFRGBGFHTp0qFPRZmZWvV2GekR0j4geEdETeB0YBBRL6pk0ORNYArwCnCKplaR2wFHAinqq2czMatC8DvtcARRJagm8CcyOiDJJd5IJ+ALguojYksc6zcwsBzmHenK2Xq5HNduLgKI81GRmZnXkDx+ZmaWIQ93MLEUc6mZmKeJQNzNLEYe6mVmKONTNzFLEoW5mliIOdTOzFHGom5mliEPdzCxFHOpmZiniUDczSxGHuplZijjUzcxSxKFuZpYiDnUzsxRxqJuZpYhD3cwsRRzqZmYp4lA3M0sRh7qZWYo41M3MUsShbmaWIrWGuqRmkqZLWibpeUlHSOosaamkJZLukVSQtB0qabmklyT1rf/yzcwsW/Mc2pwNEBHdJPUE7gAEjI2IRZKmAP0kvQiMBAqBVsBSSQsiorReKjczsypqDfWIeFzSvGTxUGANcBawOFlXDJwBlAHLkhAvlbQKOBZ4Ne9Vm5lZtXIaU4+I7ZLuByYDswFFRCSbNwLtgLbA+qzdytfvRNKwZIhm+dq1a/eoeDMz21nOF0oj4vvA/wWKgH2zNrUB1gEbkteV11c+ztSIKIyIwg4dOtShZDMzq0kuF0oHShqTLG4GdgDLk/F1gDOBJcArwCmSWklqBxwFrMh7xWZmVqNcLpT+BrhP0vNAC+BHwJtAkaSWyevZEVEm6U4yAV8AXBcRW+qnbDMzq04uF0o3Af9WzaYe1bQtIjM8Y2ZmjcAfPjIzSxGHuplZijjUzcxSxKFuZpYiDnUzsxRxqJuZpYhD3cwsRRzqZmYp4lA3M0sRh7qZWYo41M3MUsShbmaWIg51M7MUcaibmaWIQ93MmqSIYPTo0UybNg2ALVu2MGbMGPr27ctZZ53FmDFj2LJl14+EuPTSS7nxxhsrlmfNmkWvXr3o378/7777bsX6oUOH8tZbb9VPRypxqJtZk/PWW2/x/e9/n/nz51esu+eeeygrK+OJJ57giSeeoLS0lF/+8pc1HqOoqIjly5fvtG7q1Kk8+eSTDBkyhIceegiA4uJiOnfuzBFHHFE/nakklycfmZmlysyZM7ngggs46KCDKtadeOKJHHzwwRQUZM51jzrqKFatWlXt/i+//DJLlizhO9/5Dhs2bKhY36JFC0pKSti4cWPF6+nTp3PffffVb4ey+EzdzJqccePGcfbZZ++07uSTT+bwww8H4K9//Sv3338/ffr0qbLvmjVruOmmm7jtttto1qzZTtsuv/xyBg4cyIIFCxg0aBBTpkxhwIABtG7duv46U4nP1M3MsqxYsYJLL72UCy+8kFNPPXWnbdu2beOKK65gzJgxdOzYscq+vXv3pnfv3gC88847vP7664waNYqbbrqJ1atX8/Wvf52LLrqoXut3qJuZJZ588knGjx/PT37ykypn8pAJ/HfffZdbbrkFgA8++ICysjJKS0u56aabdmo7ceJERo8ezQsvvMCmTZuYOnUqgwcP5hvf+AaHHnpovfXBoW5mBjz77LNMmDCBadOm0bVr12rbfOUrX2Hx4sUVy5MnT+af//wn48aN26ndc889xwEHHMDRRx/Ns88+S/PmzZGEpFrvqNlTDnUzM+DWW28lIhg7dmzFuuOPP57rr7+eSZMmATBq1Khaj7N161buvvtuioqKgMxY/cyZMzn99NM56aST6NKlS/10IKGIqHmj1AKYDhwG7ANMAP4XmAEEsAIYERE7JA0FfghsByZExLza3rywsDAq3xJkVtm1dy9r7BIa1M2XdGuSfbbdopo21HamfiHwYUQMlPQZ4DXgdWBsRCySNAXoJ+lFYCRQCLQClkpaEBGleSnfzMxyUluoPwLMzlreDpwAlA8qFQNnAGXAsiTESyWtAo4FXs1vuWZmtiu7vE89Ij6OiI2S2pAJ97FkhmzKx2w2Au2AtsD6rF3L11chaZik5ZKWr127do87YGafftu272jsEhpcffW51gulkjoBjwF3R8RDkv4za3MbYB2wIXldeX0VETEVmAqZMfU6VW1mqdKieYGvI+TJLs/UJR0APAOMjojpyerXJPVMXp8JLAFeAU6R1EpSO+AoMhdRzcysAdV2pn4tsD/wE0k/SdaNAu6U1BJ4E5gdEWWS7iQT8AXAdRFRvzdjmplZFbsM9YgYRSbEK+tRTdsioChPdZmZWR14Qi8zsxRxqJuZpYhD3cwsRRzqZmYp4lA3M0sRh7qZWYo41M3MUsShbmaWIg51M7MUcaibmaWIQ93MLEUc6mZmKeJQNzNLEYe6mVmKONTNzFLEoW5mliIOdTOzFHGom5mliEPdzCxFHOpmZiniUDczSxGHuplZijjUzcxSJKdQl/SvkhYlrztLWippiaR7JBUk64dKWi7pJUl967FmMzOrQa2hLulq4F6gVbLqDmBsRJwCCOgn6UBgJNAN6A1MlLRP/ZRsZmY1yeVM/S2gf9byCcDi5HUx0Av4KrAsIkojYj2wCji2uoNJGpac0S9fu3Zt3Ss3M7Mqag31iHgU2Ja1ShERyeuNQDugLbA+q035+uqONzUiCiOisEOHDnWr2szMqlWXC6U7sl63AdYBG5LXldebmVkDqkuovyapZ/L6TGAJ8ApwiqRWktoBRwEr8lKh7ZUefPBBzjrrLPr27cu///u/8+GHH+bc5p133qFfv36cccYZzJ49u6L9448/zs9//vOG6oJZKtUl1K8Axkt6EWgJzI6IvwN3kgn4Z4HrImJL/sq0vcmKFSuYPn06s2bNYt68eRx22GFMmjQp5zYzZ85k6NChzJs3jylTpgDw8ccf89BDD/HDH/6wwftjlibNc2kUEauBryWv/wj0qKZNEVCUz+Js73TMMccwf/58WrRoQWlpKWvWrOHzn/98zm1atmzJ5s2b2bx5MwUFmfOKX/ziFwwePJh99923wftjlib+8JHVSYsWLVi4cCHdu3fn1VdfpX///jm3GThwIE899RSDBg3i6quv5q233uJPf/oTffr0aehumKVOTmfqZtXp1asXvXr14te//jVDhgxhwYIFFWfeu2rTsWNHZsyYUdHm4osvZsyYMSxatIiHHnqI1q1bM27cONq3b9+wHTJLAZ+p2257++23Wb58ecXyeeedx/vvv8/69et3qw1AcXExRxxxBJ07d2bixIlMmjSJHj167BT6ZpY7h7rttrVr13L55Zfz0UcfATB37lyOPPJI9t9//91qU1JSwrRp07jssssA2L59OwUFBRQUFLBli6+zm9WFh19stxUWFjJ8+HAGDRpEs2bN6NixI3fddRdvvPEGY8eOZc6cOTW2yTZlyhQGDBhA69atARg8eDBnnXUWbdu2rXI3jZnlRp98OLThFRYWRvav6GbVufbuZY1dQoO6+ZJu7nMTcPMl3fZkd9W0wcMvZmYp4lA3M0sRh7qZWYo41D9Ftm3fUXsjM2vSfPdLHixatIjbb7+drVu30qVLF26++eaKOzrKrVy5kgkTJrBx40YKCgq48cYbOeaYY3jnnXe47LLLKCkpYdiwYZx//vlAZnKr1atX86Mf/ajiGC2aF/hikpntks/U99BHH33EmDFjmDx5MvPnz6dTp07cdtttO7UpKSlhyJAhXHzxxTz++ONccsklXHnllYAntzKz/HKo76GlS5fStWtXDjvsMAC++93vMnfuXLJvFV22bBmdOnWiR4/MPGinnXZaxRSzntzKzPLJob6H/v73v3PggQdWLB944IF8/PHHbNq0qWLdX/7yFzp06MC1115L//79ueiiiygrKwM8uZWZ5ZfH1PfQjh07kKp+DiB7Yqvt27ezePFiHnjgAY477jgWLlzIsGHDeO655zy5lZnllc/U99DnPvc5/vGPf1Qsr1mzhnbt2rHffvtVrOvYsSNHHHEExx13HJCZubCsrIx33313p2N5cisz21MO9T108skn87vf/Y7Vq1cDMGvWLE477bSd2nTv3p333nuPFSsyT/h79dVXkbTTgyU8uZWZ5YOHX/bQZz7zGSZOnMjIkSPZtm0bhxxyCLfeeutOk1t16NCBu+66i/Hjx1NSUkLLli2ZPHky++yzT8VxPLmVmeWDQz0PevToUXFnS7n27dszZ86ciuUTTzyRRx55pMZj/PjHP95pecCAAQwYMCC/hZpZ6nn4xcwsRRzqZmYp4lA3M0uRvIa6pAJJUyS9KGmRpM75PH42T25lZlZVvi+UfgtoFREnSfoacDvQL8/vAXhyKzOz6uR7+OVk4GmAiHgJKMzz8c3MbBfy+oxSSfcCj0ZEcbL8DvCFiNie1WYYMCxZ7AKszFsBDeezwAeNXUQDa2p9bmr9Bff50+SDiKh2gqh8D79sANpkLRdkBzpAREwFpub5fRuUpOUR0aR+C2lqfW5q/QX3OS3yPfyyDPgmQDKm/kaej29mZruQ7zP1x4DTJb0ACLgoz8c3M7NdyGuoR8QOYHg+j7mX+lQPH9VRU+tzU+svuM+pkNcLpWZm1rj8iVIzsxRxqOdIUjNJ0yUtk/S8pCMau6aGIqmjpHclfbGxa2kIkl5LPhG9SNJ9jV1PQ5A0Jvkk+G8lDWnseuqbpB9k/R2/JGmLpPaNXVc+eOrd3J0NEBHdJPUE7qCePi27N5HUAvglUNLYtTQESa0AIqJnI5fSYJJ/z18HugH7AVc2Zj0NISJmADMAJN0FTI+IdY1YUt74TD1HEfE4n3xo6lBgTeNV06BuA6YA7zd2IQ3kOGA/Sc9Ieja5NTftepO5/fgxYC4wr3HLaTiSCoEvJZ+fSQWH+m6IiO2S7gcmA7Mbu576JukHwNqImN/YtTSgzWR+kPUmcyfXTElp/432s2Sm9LiAT/pc9Wnq6XQtML6xi8gn3/1SB5IOBF4Gjo6ITY1dT32R9DwQydeXgT8C50TE3xuzrvokaR8yn4QuSZZfAc6LiHd3veenl6RbyPzwvj1Z/h1wekT8Y9d7frolY+gvRMTRjV1LPvlMPUeSBkoakyxuBnYAZY1YUr2LiO4R0SMZX34dGJTmQE8MJjO7KJIOAtoCf2vUiurfUqCPMg4C/gX4sJFragjdgYWNXUS+pf3Xynz6DXBfcvbaAvhRRGxp5Jos/6YBMyQtJfMbyuDK8xelTUTMk9QdeIXMid6IiEj1CUuiC/Dnxi4i3zz8YmaWIh5+MTNLEYe6mVmKONTNzFLEoW5mliIOdTOzFHGom+2CpBskDZf0ZUnjknXnJvdzm+11HOpmOYiI1yPixmRxFJkPJZntdRzqlmqSWkt6QtISSVMk/T6ZbvWLyfbhkm5IXk+UtCCZivW+SsfpKWmWpLPITJnwgKRhkn6abG8m6Y1kmgGzRuNQt7S7BHgjIk4BHqCGM2xJbYF/RsTpZKah/Zqkgyu3i4gnSaZMAB4GviWpGdAHeC4iSuulF2Y58jQBlnaHA08DRMQLkipP7VA+G2EJ0FHSw8DHQGsy00HUKCI2SlpMZkbHi4Abd9XerCH4TN3S7vdkHv6ApK5AK2AL8Llk+/HJn2cCnSLiu2SmY92XTwK/sh188n+nCLgY6BgRv8979Wa7yaFuaXcvcGAyEdvVybo7gbskzQeaJeteAb4g6SUyc+X/GajpDpcXyIyp/5+IeBnoDMysrw6Y7Q5P6GVNRvKouv8XEYfl8ZgFwDKgd0RsyNdxzerKZ+pmdSTpcOB/gAcc6La38Jm6mVmK+EzdzCxFHOpmZiniUDczSxGHuplZijjUzcxSxKFuZpYi/x/FlEP2QQvlRgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "fig.suptitle('Target distribution', fontsize=16)\n",
    "sns.set()\n",
    "\n",
    "createSubplotHist(df_wine_quality, \"quality\", ax, np.array(sorted(df_wine_quality[\"quality\"].unique()))-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ca26a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into a train, test and validation split\n",
    "\n",
    "df_train = df_wine_quality.sample(frac=0.7, random_state=1) # use 70% of the dataset for the training set \n",
    "df_wine_quality.drop(np.array(df_train.index), inplace=True) # drop the used entries from the original dataset \n",
    "\n",
    "df_test = df_wine_quality.sample(frac=0.5, random_state=1) # use 50% of the remaining ds (15% of the original) for testing\n",
    "df_wine_quality.drop(np.array(df_test.index), inplace=True) # drop the used entries from the original dataset \n",
    "\n",
    "df_valid = df_wine_quality.copy() # use the remaining data for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d369203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the labels from the input and store them\n",
    "\n",
    "labels_train = df_train.pop('quality')\n",
    "labels_test = df_test.pop('quality')\n",
    "labels_valid = df_valid.pop('quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9beb41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build tensorflow datasets from the features and the targets\n",
    "import tensorflow as tf\n",
    "\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((df_train.values, labels_train.values))\n",
    "ds_test = tf.data.Dataset.from_tensor_slices((df_test.values, labels_test.values))\n",
    "ds_valid = tf.data.Dataset.from_tensor_slices((df_valid.values, labels_valid.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d815d7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: [ 8.8     0.41    0.64    2.2     0.093   9.     42.      0.9986  3.54\n",
      "  0.66   10.5   ], Target: 5\n",
      "Features: [ 8.7      0.63     0.28     2.7      0.096   17.      69.       0.99734\n",
      "  3.26     0.63    10.2    ], Target: 6\n",
      "Features: [10.4    0.34   0.58   3.7    0.174  6.    16.     0.997  3.19   0.7\n",
      " 11.3  ], Target: 6\n"
     ]
    }
   ],
   "source": [
    "for features, target in ds_train.take(3):\n",
    "    print('Features: {}, Target: {}'.format(features, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac9b1a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# median quality of all wines\n",
    "med = pd.read_csv(filepath_or_buffer=\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\",\n",
    "                             sep=\";\")[\"quality\"].median()\n",
    "\n",
    "# function to make the target binary\n",
    "def make_binary(target):\n",
    "    '''\n",
    "    receives a target (int)\n",
    "    returns a target fit for a binary classification task (threshold: median of the original data)\n",
    "    '''\n",
    "    temp = 1 if target > int(med) else 0\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32178a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data pipeline\n",
    "def prepare_data(ds):\n",
    "    # create binary targets\n",
    "    ds = ds.map(lambda features, target: (features, make_binary(target)))\n",
    "    # shuffle, batch and prefetch the data\n",
    "    ds = ds.shuffle(500).batch(32).prefetch(20)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71eeb2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the training and the test dataset\n",
    "train_dataset = ds_train.apply(prepare_data)\n",
    "test_dataset = ds_test.apply(prepare_data)\n",
    "valid_dataset = ds_valid.apply(prepare_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d989171",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "248deea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Layer\n",
    "class SimpleDense(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Custom layer function, which inherits from the keras.layers.Layer class\n",
    "\n",
    "    Functions:\n",
    "    init: constructor\n",
    "    call: calculates output\n",
    "    build: creates weights and bias when model is called for the first time\n",
    "    '''\n",
    "\n",
    "    def __init__(self, units, activation):\n",
    "        '''\n",
    "        Constructs a fully connected layer\n",
    "        \n",
    "        Args:\n",
    "          units (int): number of perceptrons of the layer\n",
    "          activation (tf.nn activation function): activation function of the layer\n",
    "        '''\n",
    "        super(SimpleDense, self).__init__()\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "\n",
    "    def build(self, input_shape): \n",
    "        '''\n",
    "        Creates random weights and bias from a normal distribution\n",
    "\n",
    "        Args:\n",
    "          input_shape: dimension of the input tensor\n",
    "        '''\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                               initializer='random_normal',\n",
    "                               trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                               initializer='random_normal',\n",
    "                               trainable=True)\n",
    "\n",
    "    def call(self, inputs): \n",
    "        '''\n",
    "        Calculates the output of the layer (forward step)\n",
    "\n",
    "        Args:\n",
    "          inputs: input tensor of the layer\n",
    "        \n",
    "        Returns:\n",
    "          x: output of the layer\n",
    "        '''\n",
    "\n",
    "        x = tf.matmul(inputs, self.w) + self.b\n",
    "        x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10de9575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Model\n",
    "class MyModel(tf.keras.Model):\n",
    "    \"\"\" \n",
    "    Custom MLP model for a binary classification task, which inherits from the keras.Model class\n",
    "\n",
    "    Functions:\n",
    "    init: constructor\n",
    "    call: performs forward pass\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Constructs the model with 1 hidden layer (1 perceptron) and an output layer with 1 output neuron\n",
    "        '''\n",
    "        super(MyModel, self).__init__()\n",
    "        self.dense = SimpleDense(4, activation=tf.nn.sigmoid) # hidden layer\n",
    "        self.out = SimpleDense(1, activation=tf.nn.sigmoid) # output layer with 1 unit since we have a binary task\n",
    "\n",
    "    def call(self, inputs):\n",
    "        '''\n",
    "        Performs a forward step in our MLP\n",
    "        \n",
    "        Args:\n",
    "          inputs (Tensor): input to the model\n",
    "        \n",
    "        Returns: \n",
    "          x: output of the model\n",
    "        '''\n",
    "        x = self.dense(inputs)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a606bd",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0c71e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, inputs, target, loss_function, optimizer):\n",
    "    '''\n",
    "    Forward and backward pass for one datapoint of the training set\n",
    "\n",
    "    Args:\n",
    "      model: the created tf model\n",
    "      inputs: tensor of the input features\n",
    "      target: target corresponding to the input features\n",
    "      loss_funcion: tensorflow loss function\n",
    "      optimizer: tensorflow optimizer\n",
    "    \n",
    "    Returns:\n",
    "      loss: loss for the datapoint\n",
    "    '''\n",
    "    with tf.GradientTape() as tape:\n",
    "        # forward step\n",
    "        prediction = model(inputs)\n",
    "        # calculate loss\n",
    "        loss = loss_function(target, prediction)\n",
    "        # calculate gradients\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # apply optimizer\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "def test(model, test_data, loss_function):\n",
    "    '''\n",
    "    Tests our model by performing forward pass and calculating loss and accuracy.\n",
    "\n",
    "    Args:\n",
    "      model: the created tf model\n",
    "      test_data: the prepared test dataset\n",
    "      loss_funcion: tensorflow loss function\n",
    "    \n",
    "    Returns:\n",
    "      test_loss: mean loss of this epoch\n",
    "      test_accuracy: mean accuracy of this epoch\n",
    "    \n",
    "    '''    \n",
    "    # initialize lists for accuracys and loss \n",
    "    test_accuracy_aggregator = []\n",
    "    test_loss_aggregator = []\n",
    "    \n",
    "    for (inputs, target) in test_data:\n",
    "        # forward step\n",
    "        prediction = model(inputs)\n",
    "        # calculate loss and accuracy and add them to the respective lists\n",
    "        sample_test_loss = loss_function(target, prediction)\n",
    "        sample_test_accuracy =  np.round(target, 0) == np.round(prediction,0)\n",
    "        sample_test_accuracy = np.mean(sample_test_accuracy)\n",
    "        test_loss_aggregator.append(sample_test_loss.numpy())\n",
    "        test_accuracy_aggregator.append(np.mean(sample_test_accuracy))\n",
    "    \n",
    "    # calculate the mean of the loss and accuracy (for this epoch)\n",
    "    test_loss = tf.reduce_mean(test_loss_aggregator)\n",
    "    test_accuracy = tf.reduce_mean(test_accuracy_aggregator)\n",
    "    \n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5928013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 starting with accuracy 0.84\n",
      "Epoch: 1 starting with accuracy 0.84\n",
      "Epoch: 2 starting with accuracy 0.84\n",
      "Epoch: 3 starting with accuracy 0.85\n",
      "Epoch: 4 starting with accuracy 0.84\n",
      "Epoch: 5 starting with accuracy 0.85\n",
      "Epoch: 6 starting with accuracy 0.84\n",
      "Epoch: 7 starting with accuracy 0.84\n",
      "Epoch: 8 starting with accuracy 0.84\n",
      "Epoch: 9 starting with accuracy 0.83\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate = 0.1\n",
    "\n",
    "# initialize the model\n",
    "model = MyModel()\n",
    "# initialize the loss\n",
    "cross_entropy_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "# initialize the optimizer: SGD with default parameters\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
    "\n",
    "# initialize lists for later visualization.\n",
    "train_losses = []\n",
    "\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "# testing once before we begin\n",
    "test_loss, test_accuracy = test(model, test_dataset, cross_entropy_loss)\n",
    "test_losses.append(test_loss)\n",
    "test_accuracies.append(test_accuracy)\n",
    "\n",
    "# check how model performs on train data once before we begin\n",
    "train_loss, _ = test(model, train_dataset, cross_entropy_loss)\n",
    "train_losses.append(train_loss)\n",
    "\n",
    "# we train for num_epochs epochs\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch: {0} starting with accuracy {1:.2f}'.format(epoch,test_accuracies[-1]))\n",
    "\n",
    "    # training\n",
    "    epoch_loss_agg = []\n",
    "    for input,target in train_dataset:\n",
    "        train_loss = train_step(model, input, target, cross_entropy_loss, optimizer)\n",
    "        epoch_loss_agg.append(train_loss)\n",
    "    \n",
    "    # track training loss\n",
    "    train_losses.append(tf.reduce_mean(epoch_loss_agg))\n",
    "\n",
    "    # testing\n",
    "    test_loss, test_accuracy = test(model, test_dataset, cross_entropy_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56850db",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
